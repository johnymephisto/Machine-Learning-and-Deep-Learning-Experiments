{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character Language Model",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnymephisto/Machine-Learning-and-Deep-Learning-Experiments/blob/master/Character_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk820TtIw1XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.13.1\n",
        "!pip install -q textgenrnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-_pr68e8_yE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for getting files from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3V4V-Jxmuv3",
        "outputId": "4be064cb-49f6-4dbf-db9e-98f86a4c6bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import numpy as np\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "from textgenrnn import textgenrnn as word_rnn\n",
        "print(tf.__version__)\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA7AhJDX9VT8",
        "colab_type": "code",
        "outputId": "607eb3df-fe94-4692-c552-2632a1c89ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "INPUT_TXT = 'dataset/hpotter.txt'\n",
        "def transform(txt, pad_to=None):\n",
        "  # drop any non-ascii characters\n",
        "    output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "    if pad_to is not None:\n",
        "        output = output[:pad_to]\n",
        "        output = np.concatenate([\n",
        "            np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
        "            output,\n",
        "        ])\n",
        "    return output\n",
        "\n",
        "def training_generator(seq_len=100, batch_size=1024):\n",
        "    \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
        "    with tf.gfile.GFile(INPUT_TXT, 'r') as f:\n",
        "        txt = f.read()\n",
        "\n",
        "    tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
        "    source = transform(txt)\n",
        "#     source = txt\n",
        "    while True:\n",
        "        offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
        "\n",
        "        # Our model uses sparse crossentropy loss, but Keras requires labels\n",
        "        # to have the same rank as the input logits.  We add an empty final\n",
        "        # dimension to account for this.\n",
        "        yield (\n",
        "            np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
        "            np.expand_dims(\n",
        "                np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]),\n",
        "                -1),\n",
        "        )\n",
        "\n",
        "six.next(training_generator(seq_len=10, batch_size=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 44,  32, 119, 105, 116, 104,  32,  97,  32, 112]], dtype=int32),\n",
              " array([[[ 32],\n",
              "         [119],\n",
              "         [105],\n",
              "         [116],\n",
              "         [104],\n",
              "         [ 32],\n",
              "         [ 97],\n",
              "         [ 32],\n",
              "         [112],\n",
              "         [117]]], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLEM-fLJlEEt",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "    \n",
        "    \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "    \n",
        "    source = tf.keras.Input(name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "    embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "    \n",
        "    lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "    \n",
        "    lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "    \n",
        "    predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
        "    \n",
        "    model.compile(\n",
        "      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ExQ922tfzSGA",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "training_model = lstm_model(seq_len=300, batch_size=128, stateful=False)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.fit_generator(\n",
        "    training_generator(seq_len=300, batch_size=1024),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        ")\n",
        "tpu_model.save_weights('Saved_Model/potter_1.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tU7M-EGGxR3E",
        "colab": {}
      },
      "source": [
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=1, stateful=True)\n",
        "prediction_model.load_weights('gdrive/My Drive/Saved_Model/potter.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JeCobRXm8mI",
        "colab_type": "code",
        "outputId": "fdf9f69f-8421-4c21-bdca-de9907239a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "BATCH_SIZE = 1\n",
        "PREDICT_LEN = 200\n",
        "seed_txt = 'Harry took out his wand and'\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "    prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "    last_word = predictions[-1]\n",
        "    next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "\n",
        "    # sample from our output distribution\n",
        "    next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "    ]\n",
        "    predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "    print('PREDICTION %d\\n\\n' % i)\n",
        "    p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "    generated = ''.join([chr(c) for c in p])\n",
        "    print(generated)\n",
        "    print()\n",
        "    assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            "d said, stripping through the impression and umbre to step from the ramp causing, carved around the same, and think she catch his hand she held across the room apart from the twinse faces in the effor\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVyqn0DwIxEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word model\n",
        "model_name = 'potter_model_3'\n",
        "word_gen = word_rnn(weights_path='Saved_Model/{}_weights.hdf5'.format(model_name),\n",
        "                       vocab_path='Saved_Model/{}_vocab.json'.format(model_name),\n",
        "                       config_path='Saved_Model/{}_config.json'.format(model_name))\n",
        "\n",
        "word_gen.generate(max_gen_length=200, temperature=[0.2], prefix=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzY_DH_BOFam",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF8MWdeM-HZP",
        "colab_type": "code",
        "outputId": "7f65d315-8107-4a99-defb-8edc73f676f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "INPUT_TXT = 'gdrive/My Drive/Saved_Model/elon.txt'\n",
        "def transform(txt, pad_to=None):\n",
        "  # drop any non-ascii characters\n",
        "    output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "    if pad_to is not None:\n",
        "        output = output[:pad_to]\n",
        "        output = np.concatenate([\n",
        "            np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
        "            output,\n",
        "        ])\n",
        "    return output\n",
        "\n",
        "def training_generator(seq_len=100, batch_size=1024):\n",
        "    \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
        "    with tf.gfile.GFile(INPUT_TXT, 'r') as f:\n",
        "        txt = f.read()\n",
        "\n",
        "    tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
        "    source = transform(txt)\n",
        "    while True:\n",
        "        offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
        "\n",
        "        # Our model uses sparse crossentropy loss, but Keras requires labels\n",
        "        # to have the same rank as the input logits.  We add an empty final\n",
        "        # dimension to account for this.\n",
        "        yield (\n",
        "            np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
        "            np.expand_dims(\n",
        "                np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]),\n",
        "                -1),\n",
        "        )\n",
        "\n",
        "six.next(training_generator(seq_len=10, batch_size=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 64, 107, 114, 112,  53,  32,  64, 118, 105, 110]], dtype=int32),\n",
              " array([[[107],\n",
              "         [114],\n",
              "         [112],\n",
              "         [ 53],\n",
              "         [ 32],\n",
              "         [ 64],\n",
              "         [118],\n",
              "         [105],\n",
              "         [110],\n",
              "         [ 99]]], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lFKsiBJ-Jd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "training_model = lstm_model(seq_len=100, batch_size=128, stateful=False)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.fit_generator(\n",
        "    training_generator(seq_len=100, batch_size=1024),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        ")\n",
        "tpu_model.save_weights('gdrive/My Drive/Saved_Model/elon_1.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJg2cTIM-Suv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=1, stateful=True)\n",
        "prediction_model.load_weights('gdrive/My Drive/Saved_Model/elon.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eugpdt91-RhI",
        "colab_type": "code",
        "outputId": "51165679-280b-43cd-db28-9a2479fdd05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "BATCH_SIZE = 1\n",
        "PREDICT_LEN = 200\n",
        "seed_txt = 'My lil monster tesla Model 3 is the'\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "    prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "    last_word = predictions[-1]\n",
        "    next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "\n",
        "    # sample from our output distribution\n",
        "    next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "    ]\n",
        "    predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "    print('PREDICTION %d\\n\\n' % i)\n",
        "    p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "    generated = ''.join([chr(c) for c in p])\n",
        "    print(generated)\n",
        "    print()\n",
        "    assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            "e best. Heless reliable. @Model3Owners There is a successful launch from Padie  RT @thetrKBD: .@elonmusk holding access to the video unless sent to us by owners next week. We are just being higher. I \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr5BVF7EOxzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word model\n",
        "model_name = 'elon_model_1'\n",
        "word_gen = word_rnn(weights_path='gdrive/My Drive/Saved_Model/{}_weights.hdf5'.format(model_name),\n",
        "                       vocab_path='gdrive/My Drive/Saved_Model/{}_vocab.json'.format(model_name),\n",
        "                       config_path='gdrive/My Drive/Saved_Model/{}_config.json'.format(model_name))\n",
        "\n",
        "word_gen.generate(max_gen_length=50, temperature=[0.2], prefix=None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}