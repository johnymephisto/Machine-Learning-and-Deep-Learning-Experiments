{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adanet - Implementation for text classification with tf-hub",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnymephisto/Machine-Learning-and-Deep-Learning-Experiments/blob/master/Adanet_Implementation_for_text_classification_with_tf_hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DisywrOb1fPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Adanet implementation with TF-Hub for multiclass text classification\n",
        "\n",
        "[Adanet](https://arxiv.org/pdf/1607.01097.pdf) is a framework that uses ensemble learning.\n",
        " This model used ensemble of 2 DNN architectures that use two different text embeddings formed from tf-hub modules.\n",
        " \n",
        " The dataset used for this sample implementation is the stackoverflow dataset."
      ]
    },
    {
      "metadata": {
        "id": "ugjAtj1XgJN7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#install adanet and tensorflow version\n",
        "!pip install adanet\n",
        "!pip install tensorflow==1.10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lbey7nPN2ZqK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download the [dataset](https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv) and import the required modules."
      ]
    },
    {
      "metadata": {
        "id": "3X9VhciQORAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import urllib\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "urllib.request.urlretrieve(\"https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv\", 'stack-overflow-data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gC_bgwu92jbQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Remove the empty rows not having tags"
      ]
    },
    {
      "metadata": {
        "id": "gfY9Jv29QkCd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('stack-overflow-data.csv')\n",
        "df = df[pd.notnull(df['tags'])]\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yOVKPJzK2xWI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plot the distribution of classes and their instances. This dataset is well balanced. No need to do any sort of oversampling of undersampling."
      ]
    },
    {
      "metadata": {
        "id": "FS373OoaQ0XC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_tags = ['java','html','asp.net','c#','ruby-on-rails','jquery','mysql','php','ios','javascript','python','c','css','android','iphone','sql','objective-c','c++','angularjs','.net']\n",
        "plt.figure(figsize=(10,4))\n",
        "df.tags.value_counts().plot(kind='bar');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GF1tbc5NQ_Wq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_plot(index):\n",
        "    example = df[df.index == index][['post', 'tags']].values[0]\n",
        "    if len(example) > 0:\n",
        "        print(example[0])\n",
        "        print('Tag:', example[1])\n",
        "        \n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text\n",
        "\n",
        "df['post'] = df['post'].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uc9nZIEw3JH3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split the train and test set"
      ]
    },
    {
      "metadata": {
        "id": "IeHwoh6FRoIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.post\n",
        "y = df.tags\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwFymGiv3Lzl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Development\n",
        "\n",
        "Setup encoders with tf-hub"
      ]
    },
    {
      "metadata": {
        "id": "c0epztmPfW6W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import urllib\n",
        "\n",
        "import adanet\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#directory for saving exports\n",
        "model_dir='model'\n",
        "batch_size=64\n",
        "total_steps=40000\n",
        "\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit_transform(np.array(y_train))\n",
        "train_encoded = encoder.transform(y_train)\n",
        "test_encoded = encoder.transform(y_test)\n",
        "print(encoder.classes_)\n",
        "num_classes = len(encoder.classes_)\n",
        "\n",
        "\n",
        "ndim_embeddings = hub.text_embedding_column(\n",
        "  \"ndim\",\n",
        "  module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\", trainable=False \n",
        ")\n",
        "encoder_embeddings = hub.text_embedding_column(\n",
        "  \"encoder\", \n",
        "  module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\", trainable=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8rUYvDjN44Pa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the input functions for train, eval and also serving input function for prediction mode."
      ]
    },
    {
      "metadata": {
        "id": "g42GlZxn32b0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_features = {\n",
        "  \"ndim\": X_train,\n",
        "  \"encoder\": X_train\n",
        "}\n",
        "\n",
        "train_labels = np.array(train_encoded).astype(np.int32)\n",
        "\n",
        "eval_features = {\n",
        "  \"ndim\": X_test,\n",
        "  \"encoder\": X_test\n",
        "}\n",
        "\n",
        "eval_labels = np.array(test_encoded).astype(np.int32)\n",
        "\n",
        "def input_fn_train():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "    dataset = dataset.repeat().shuffle(100).batch(batch_size)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    data, labels = iterator.get_next()\n",
        "    return data, labels\n",
        "\n",
        "def input_fn_eval():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eval_features, eval_labels))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    data, labels = iterator.get_next()\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def serving_input_fn():\n",
        "    feature_placeholders = {\n",
        "      'encoder' : tf.placeholder(tf.string, [None]),\n",
        "      'ndim' : tf.placeholder(tf.string, [None])\n",
        "    }\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(feature_placeholders, feature_placeholders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "limg9N_W5DIS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the estimators and the adanet ensemble estimator.\n",
        "\n",
        "Train and evaluate the estimator."
      ]
    },
    {
      "metadata": {
        "id": "Ou4q6tVk3r0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_class_head = tf.contrib.estimator.multi_class_head(\n",
        "  len(encoder.classes_),\n",
        "  loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
        ")\n",
        "\n",
        "estimator_ndim = tf.contrib.estimator.DNNEstimator(\n",
        "  head=multi_class_head,\n",
        "  hidden_units=[64,10],\n",
        "  feature_columns=[ndim_embeddings]\n",
        ")\n",
        "\n",
        "estimator_encoder = tf.contrib.estimator.DNNEstimator(\n",
        "  head=multi_class_head,\n",
        "  hidden_units=[64,10],\n",
        "  feature_columns=[encoder_embeddings]\n",
        ")\n",
        "\n",
        "# AutoEnsembleEstimator for adanet\n",
        "estimator = adanet.AutoEnsembleEstimator(\n",
        "    head=multi_class_head,\n",
        "    candidate_pool=[\n",
        "        estimator_encoder,\n",
        "        estimator_ndim\n",
        "    ],\n",
        "    config=tf.estimator.RunConfig(\n",
        "      save_summary_steps=1000,\n",
        "      save_checkpoints_steps=1000,\n",
        "      model_dir=model_dir\n",
        "    ),\n",
        "    max_iteration_steps=5000)\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "  input_fn=input_fn_train,\n",
        "  max_steps=total_steps\n",
        ")\n",
        "\n",
        "eval_spec=tf.estimator.EvalSpec(\n",
        "  input_fn=input_fn_eval,\n",
        "  steps=None,\n",
        "  start_delay_secs=10,\n",
        "  throttle_secs=10\n",
        ")\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AN-wlXSU5s9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Export the estimator "
      ]
    },
    {
      "metadata": {
        "id": "x1aSx_8G4ptq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latest_ckpt = tf.train.latest_checkpoint(model_dir)\n",
        "last_eval = estimator.evaluate(\n",
        "  input_fn_eval,\n",
        "  checkpoint_path=latest_ckpt\n",
        ")\n",
        "\n",
        "# Export the model to GCS for serving\n",
        "exporter = tf.estimator.LatestExporter('exporter', serving_input_fn, exports_to_keep=None)\n",
        "exporter.export(estimator, model_dir, latest_ckpt, last_eval, is_the_final_export=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}